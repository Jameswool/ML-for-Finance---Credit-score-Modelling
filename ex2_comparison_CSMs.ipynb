{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as sk\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EX2. Comparison of different Credit Score Models performance on a public dataset\n",
    "\n",
    "We compare different credit score models on teh database train.csv obtained by Kaggle at the link https://www.kaggle.com/competitions/credit-default-prediction-ai-big-data/data?select=test.csv. The name of the dataset is 'db_exercise_2_CSMs.csv'\n",
    "\n",
    "0. split the dataset in training set and numerical set, with a proportion 0.75 - 0.25\n",
    "\n",
    "1. EDA (exploratory data analysis):\n",
    "    For the training set and the test set provides separately the following steps: \n",
    "    1. converting categorical data in numeric when possible; \n",
    "    2. missing values: substitute the missing value of every numerical variable with the median value on the dataset; for the categoric variable substitute the missing values with the most present value on the dataset. \n",
    "    3. outliers: for every numerical variable that takes continuous values: substitute the value beyond the 97th percentile and the 3rd percentile with the 97th percentile and the 3rd percentile. We say that a feature takes continuous values if it takes more than 1000 different values; \n",
    "    4. Data visualization:\n",
    "        1. For every categorical feature provide the distribution of default: plot the distribution of the default on every value of every categoric variable\n",
    "        2. Provide the correlation matrix between the numerical features. \n",
    "\n",
    "2. Apply Smote algorithm to the training set to manage the imbalanced classes.\n",
    "\n",
    "3. Model estimation. First of all, exclude the feature \"Credit Score\" from the selection (in point 4. we are going to see the it has high correlation with the credit_default variable). For every model in the following list, provide the best hyper-parameters in terms of f1 score. To do so, apply GridSearchCV to the list of parameters gives in the following dictionary. Then fit the obtained model to the training set, both in the case in which the training set is not handled by Smote algorithm, as well as in the case in which it is:\n",
    "    1. KNN;\n",
    "    2. Logistic Regression;\n",
    "    3. Decision Tree;\n",
    "    4. Random Forest;\n",
    "    5. Adaboost;\n",
    "    6. XGBoost.\n",
    "    \n",
    "\n",
    "4. Model evaluation. Provide the following performance score. Compare the Roc curve of every model in the same figure. \n",
    "    1. Accuracy;\n",
    "    2. Precision;\n",
    "    3. Recall;\n",
    "    4. F-score;\n",
    "    5. ROC curve;\n",
    "    6. AUC.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data: \n",
    "\n",
    "random_state = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dictio_parameters = {\n",
    "                        'knn' : {'n_neighbors' : list(range(1,101,10))},\n",
    "                        'logistic_regression': dict(zip(['tol', 'C', 'max_iter','solver'], \n",
    "                                                    [ \n",
    "                                                        [10**(-i) for i in range(1,7)],\n",
    "                                                        [0.001,0.01,0.1,1,10,100],\n",
    "                                                        [100*i for i in range(1,11)],\n",
    "                                                        ['liblinear']\n",
    "                                                        ])),\n",
    "                        'decision_tree_classifier' : dict(zip(  ['max_depth','min_samples_split','min_samples_leaf'],\n",
    "                                                    [   None,\n",
    "                                                        [2,3,4,5],\n",
    "                                                        [1,2,3,4]])),\n",
    "                        'Random_forest_classifier' : dict(zip(['n_estimators','max_depth','min_samples_split','min_samples_leaf'],\n",
    "                                                    [   [100,200,300],\n",
    "                                                        None,\n",
    "                                                        [2,3,4],\n",
    "                                                        [1,2,3]] )),\n",
    "                        'Adaboost_Classifier' : dict(zip(['n_estimators', 'learning_rate'], \n",
    "                                                    [   [100,200,300],\n",
    "                                                        [0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3]])),\n",
    "                        'XGboost_Classifier' : dict(zip(['n_estimators','max_depth','learning_rate','gamma'], \n",
    "                                                    [   [100,200,300],\n",
    "                                                        None,\n",
    "                                                        [0.6,0.7,0.8,0.9,1.0,1.1,1.2,1.3],\n",
    "                                                        [1,0.1,0.01,0]\n",
    "                                                        ]))}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('train.csv')\n",
    "#df.to_csv('db_exercise_2_CSMs.csv')\n",
    "df = pd.read_csv('db_exercise_2_CSMs.csv').drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "default_variable = 'Credit Default'\n",
    "id_variable = 'Id'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
